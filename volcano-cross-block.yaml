apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: nccl-alltoall-job-oguz
spec:
  minAvailable: 1
  schedulerName: volcano
  plugins:
    ssh: []
    svc: []
  queue: default
  tasks:
    - replicas: 1
      name: mpimaster
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        spec:
          volumes:
            - name: topo
              configMap:
                name: nccl-topology
                items:
                - key: topo.xml
                  path: topo.xml
            - name: root
              hostPath:
                path: /
                type: Directory
          initContainers:
            - command:
                - /bin/bash
                - -c
                - |
                  until [[ "$(kubectl get pod -l volcano.sh/job-name=nccl-alltoall-job-oguz,volcano.sh/task-spec=mpiworker -o json | jq '.items | length')" != 0 ]]; do
                    echo "Waiting for MPI worker pods..."
                    sleep 3
                  done
                  echo "Waiting for MPI worker pods to be ready..."
                  kubectl wait pod -l volcano.sh/job-name=nccl-alltoall-job-oguz,volcano.sh/task-spec=mpiworker --for=condition=Ready --timeout=600s && sleep 2
              image: aga.ocir.io/hpc_limited_availability/oke/kubectl:latest
              name: wait-for-workers
          serviceAccount: mpi-worker-view
          terminationGracePeriodSeconds: 2
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
          containers:
            - command:
                - /bin/bash
                - -c
                - |
                  MPI_HOST=$(cat /etc/volcano/mpiworker.host | tr "\n" ",")
                  mkdir -p /var/run/sshd; /usr/sbin/sshd
                  mpirun --allow-run-as-root \
                    -mca coll ^hcoll \
                    -np 256 -npernode 8 --bind-to numa \
                    -hostfile /etc/volcano/mpiworker.host \
                    -x NCCL_CROSS_NIC=0 \
                    -x NCCL_SOCKET_NTHREADS=16 \
                    -x NCCL_DEBUG=INFO \
                    -x NCCL_CUMEM_ENABLE=0 \
                    -x NCCL_IB_SPLIT_DATA_ON_QPS=0 \
                    -x NCCL_IB_QPS_PER_CONNECTION=16 \
                    -x NCCL_IB_GID_INDEX=3 \
                    -x NCCL_IB_HCA=mlx5 \
                    -x NCCL_IB_TC=41 \
                    -x NCCL_IB_SL=0 \
                    -x NCCL_IB_TIMEOUT=22 \
                    -x NCCL_NET_PLUGIN=none \
                    -x HCOLL_ENABLE_MCAST_ALL=0 \
                    -x coll_hcoll_enable=0 \
                    -x UCX_TLS=tcp \
                    -x UCX_NET_DEVICES=eth0 \
                    -x RX_QUEUE_LEN=8192 \
                    -x IB_RX_QUEUE_LEN=8192 \
                    -x NCCL_SOCKET_IFNAME=eth0 \
                    -x NCCL_IGNORE_CPU_AFFINITY=1 \
                    -x NCCL_TOPO_FILE=/h100/topo.xml \
                    -mca coll_hcoll_enable 0 \
                    /workspace/nccl-tests/build/alltoall_perf -b 8 -f 2 -g 1 -e 4G -c 1; sleep 3600
              #image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-23.04-nccl-2.19.3-1
              image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-23.12
              volumeMounts:
              - { mountPath: /h100, name: topo }
              - { mountPath: /host, name: root }
              securityContext:
                capabilities:
                  add: ["IPC_LOCK"]
              name: mpimaster
              ports:
                - containerPort: 22
                  name: mpijob-port
              workingDir: /workspace
              resources:
                requests:
                  cpu: 2
          restartPolicy: OnFailure
    - replicas: 32
      minAvailable: 32
      name: mpiworker
      template:
        metadata:
          annotations:
            k8s.v1.cni.cncf.io/networks: oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov,oci-rdma-sriov
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: oci.oraclecloud.com/host.serial_number
                    operator: In
                    values:
                    - 2339xlg02v
                    - 2339xlg00t
                    - 2339xlg04g
                    - 2339xlg049
                    - 2339xlg03d
                    - 2339xlg00r
                    - 2339xlg046
                    - 2339xlg03v
                    - 2339xlg07g
                    - 2339xlg04j
                    - 2339xlg05b
                    - 2337xlg037
                    - 2339xlg04n
                    - 2339xlg07u
                    - 2339xlg04y
                    - 2339xlg06j
                    - 2349xlg020
                    - 2339xlg08k
                    - 2339xlg08r
                    - 2339xlg05y
                    - 2349xlg046
                    - 2350xlg004
                    - 2349xlg05n
                    - 2342xlg006
                    - 2348xlg06d
                    - 2349xlg04w
                    - 2349xlg0b4
                    - 2337xlg05w
                    - 2342xlg00e
                    - 2349xlg025
                    - 2339xlg02u
                    - 2350xlg051
          containers:
            - name: mpiworker
              command:
                - /bin/bash
                - -c
                - mkdir -p /var/run/sshd; /usr/sbin/sshd -D;
              image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-23.12
              #image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-23.04-nccl-2.19.3-1
              securityContext:
                capabilities:
                  add: ["IPC_LOCK"]
              ports:
                - containerPort: 22
                  name: mpijob-port
              workingDir: /workspace
              resources:
                requests:
                  nvidia.com/gpu: 8
                  nvidia.com/sriov_rdma_vf: 16
                  ephemeral-storage: 1Gi
                limits:
                  nvidia.com/gpu: 8
                  nvidia.com/sriov_rdma_vf: 16
                  ephemeral-storage: 1Gi
              volumeMounts:
              - { mountPath: /h100, name: topo }
              - mountPath: /dev/shm
                name: shm
          restartPolicy: OnFailure
          terminationGracePeriodSeconds: 15
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
            - key: oguz_test
              operator: Exists
          volumes:
          - name: topo
            configMap:
              name: nccl-topology
              items:
              - key: topo.xml
                path: topo.xml
          - name: root
            hostPath:
              path: /
              type: Directory
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: 8Gi
